{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeqYa9bzWObq"
      },
      "source": [
        "# 📍 NutriChef AI - Phase 2: Gradio Frontend App\n",
        "This notebook creates the frontend UI for NutriChef AI using Gradio.\n",
        "Users can upload a fridge photo or record their voice to get meal plans, recipes, nutrition facts, and health advice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLF6nJbRtX-Q",
        "outputId": "b280643a-2020-40bd-e178-ecf67eaf40ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzNDaIzQWvlN"
      },
      "source": [
        "## 📍Step 1: Install and Import Basic Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to project folder (only if you're on Colab and this path is correct)\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/NutriChefAI\n",
        "\n",
        "# Whisper (speech-to-text from audio/video)\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "# YouTube downloader\n",
        "!pip install yt-dlp\n",
        "\n",
        "# LangChain ecosystem + LangSmith for tracing\n",
        "!pip install langchain langchain-community langchain-openai langsmith\n",
        "\n",
        "# Core model libraries\n",
        "!pip install openai huggingface_hub transformers\n",
        "\n",
        "# Vector database + embedding models\n",
        "!pip install chromadb sentence-transformers\n",
        "\n",
        "# Gradio for app UI\n",
        "!pip install gradio\n",
        "\n",
        "!pip install -q gradio python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXpUZuc7brrt",
        "outputId": "7ebddf28-3b25-4238-e2d0-64c0a7689823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/NutriChefAI\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ud7uugkw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ud7uugkw\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.4.30)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.39)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.76.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.8)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.4)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.33.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.9)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AdRHgZjWYWL"
      },
      "source": [
        "### Import Needed Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF8kEe8zMU62"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # Set LangSmith credentials\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"NutriChefAI\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FAzLEyINj9T"
      },
      "outputs": [],
      "source": [
        "# from langsmith import Client\n",
        "\n",
        "# client = Client()\n",
        "# project = client.read_project(project_name=\"NutriChefAI\")\n",
        "# print(f\"✅ LangSmith connected to project: {project.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Tell dotenv exactly where the .env file is\n",
        "load_dotenv(dotenv_path=\"/content/drive/MyDrive/Colab Notebooks/NutriChefAI/.env\")\n",
        "\n",
        "# ✅ Optional: Check if loaded correctly\n",
        "print(os.getenv(\"OPENAI_API_KEY\")[:8])  # Should print 'sk-proj-' or similar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjemNKCx_m98",
        "outputId": "6f9d1d59-bb41-4797-c470-7525cbc56668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sk-proj-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "client = OpenAI()  # Uses .env key\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
      ],
      "metadata": {
        "id": "VmfKWDfG_p4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from dotenv import load_dotenv\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# # Load values from .env file into environment\n",
        "# load_dotenv()\n",
        "\n",
        "# # Now you can use the variables normally\n",
        "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# # These will automatically be used by OpenAI / LangChain if you initialized clients\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# # LangChain will also use env vars if not passed directly"
      ],
      "metadata": {
        "id": "Pwur3a2x_tSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20wrh-vEZWnO"
      },
      "outputs": [],
      "source": [
        "# === Imports ===\n",
        "import gradio as gr\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import json\n",
        "import yt_dlp\n",
        "import whisper\n",
        "import ast\n",
        "import re\n",
        "import chromadb\n",
        "import pandas as pd\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from chromadb.config import Settings\n",
        "from chromadb import PersistentClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langsmith import traceable\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain.tools import Tool\n",
        "\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from chromadb.errors import NotFoundError\n",
        "\n",
        "# Import backend modules\n",
        "from models.vision_model import IngredientDetector\n",
        "# from models.speech_to_text import SpeechToText\n",
        "from models.meal_generator import create_daily_meal_plan\n",
        "from models.food_classifier import FoodClassifier\n",
        "# from utils.recipe_retriever import embed_query, search_recipes\n",
        "# from utils.nutrition_generator import generate_nutrition_facts_and_advice\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KESIs8fpZk2a"
      },
      "source": [
        "### Setup Clients and Models Once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjo6rGWKlAlC"
      },
      "outputs": [],
      "source": [
        "class SpeechToText:\n",
        "    def __init__(self):\n",
        "        import whisper\n",
        "        self.model = whisper.load_model(\"base\")\n",
        "\n",
        "    def extract_ingredients(self, audio_file_path):\n",
        "        try:\n",
        "            result = self.model.transcribe(audio_file_path)\n",
        "            text = result[\"text\"]\n",
        "            ingredients = [x.strip() for x in text.split(\",\") if x.strip()]\n",
        "            return ingredients\n",
        "        except Exception as e:\n",
        "            print(f\"🔥 Error transcribing audio: {e}\")\n",
        "            return []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rm -rf ./database/vector_store"
      ],
      "metadata": {
        "id": "UjNcliNFWlSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load both datasets\n",
        "recipes_df = pd.read_csv(\"./database/recipes.csv\")\n",
        "raw_df = pd.read_csv(\"./database/RAW_recipes.csv\", usecols=[\n",
        "    \"name\", \"minutes\", \"nutrition\", \"steps\", \"description\", \"ingredients\"\n",
        "])\n",
        "\n",
        "# Standardize both\n",
        "recipes_df[\"recipe_name\"] = recipes_df[\"recipe_name\"].str.lower().str.strip()\n",
        "recipes_df = recipes_df.rename(columns={\"recipe_name\": \"name\"})\n",
        "recipes_df[\"nutrition\"] = recipes_df[\"nutrition\"].apply(str)\n",
        "\n",
        "raw_df[\"name\"] = raw_df[\"name\"].str.lower().str.strip()\n",
        "raw_df[\"nutrition\"] = raw_df[\"nutrition\"].apply(lambda x: str(x))\n",
        "\n",
        "# Combine\n",
        "combined_df = pd.concat([recipes_df, raw_df], ignore_index=True)\n",
        "\n",
        "# Use combined_df in your lookup\n",
        "\n"
      ],
      "metadata": {
        "id": "O7pjKk2CDV-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All necessary imports here\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from chromadb import PersistentClient\n",
        "from chromadb.errors import NotFoundError\n",
        "\n",
        "import whisper\n",
        "from openai import OpenAI\n"
      ],
      "metadata": {
        "id": "9OPzbHd7AbzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup reusable models\n",
        "vision_detector = IngredientDetector()\n",
        "speech_detector = SpeechToText()\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "client = OpenAI()\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "food_detector = FoodClassifier()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjWGi8-vAdlP",
        "outputId": "4b93dcae-3bfa-43ea-ef97-7713b555619f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-13-be86c85ddd7a>:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/convnext/feature_extraction_convnext.py:28: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = PersistentClient(path=\"./database/vector_store\")\n",
        "recipe_collection = chroma_client.get_or_create_collection(name=\"recipes\")\n",
        "video_collection = chroma_client.get_or_create_collection(name=\"youtube_videos\")\n",
        "\n",
        "collection = recipe_collection  # ✅ this line fixes your Gradio error\n"
      ],
      "metadata": {
        "id": "-QctXpqSAgpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_histories = {}\n"
      ],
      "metadata": {
        "id": "oEpbh_ChOBvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Prompt template\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "You are NutriChef 🍳 — a world-class AI chef assistant.\n",
        "Speak like a real chef: helpful, confident, and clear.\n",
        "\n",
        "You remember the full conversation: dishes, ingredients, preferences, and goals.\n",
        "Handle follow-up questions like \"how to make it\" or \"is it healthy?\" based on the most recently discussed dish.\n",
        "If no dish is clear, ask politely.\n",
        "\n",
        "Use step-by-step instructions, emojis when helpful 🍚🔥🥘, and stay friendly like a real chef would.\n",
        "\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # ✅ Must match the memory key\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# 🔁 Chain with memory\n",
        "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
        "memory_chain = RunnableWithMessageHistory(\n",
        "    chat_prompt | chat_model,\n",
        "    lambda session_id: session_histories.setdefault(session_id, InMemoryChatMessageHistory()),\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "\n",
        "# Gradio States (for memory)\n",
        "ingredients_state = gr.State(\"\")\n",
        "meal_plan_state = gr.State({})"
      ],
      "metadata": {
        "id": "vyFkx_BfH19E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "def get_session_id():\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "# initial_session_id = get_session_id()\n",
        "# session_id = gr.State(initial_session_id)\n",
        "# chat_history = gr.State([])\n",
        "# last_dish_state = gr.State(\"\")"
      ],
      "metadata": {
        "id": "IL7g7MibH7g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASqv3rgQZw5D"
      },
      "source": [
        "## 📍Step 2: Write Backend Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_dish_name(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts a likely dish or food item from the user message using general NLP rules.\n",
        "    \"\"\"\n",
        "    user_message = user_message.lower()\n",
        "\n",
        "    # Explicit patterns\n",
        "    patterns = [\n",
        "        r\"(?:how to make|recipe for|cook|prepare|want(?: to try)?|suggest(?: me)?|make)\\s+([a-zA-Z\\s]+)\",\n",
        "        r\"(?:i(?:'d)? like|give me|something like)\\s+([a-zA-Z\\s]+)\",\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, user_message)\n",
        "        if match:\n",
        "            candidate = match.group(1).strip()\n",
        "            candidate = re.sub(r\"\\b(for dinner|for lunch|please|recipe)?\\b\", \"\", candidate).strip()\n",
        "            if 1 <= len(candidate.split()) <= 6:\n",
        "                return candidate\n",
        "\n",
        "    # Fallback: last 2–4 words as a potential noun phrase\n",
        "    tokens = user_message.split()\n",
        "    if len(tokens) >= 2:\n",
        "        fallback = \" \".join(tokens[-4:])  # last 4 words\n",
        "        return fallback.strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def detect_dish_with_llm(message):\n",
        "    prompt = f\"\"\"\n",
        "You're a food assistant. Extract the specific dish or food name the user is referring to in this message:\n",
        "\n",
        "\"{message}\"\n",
        "\n",
        "If no dish is mentioned, return \"none\".\n",
        "Only return the dish name.\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "kyQCDLCjE74N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1L2UUC481Zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53a6782-7d45-47cb-86bb-01e620bd55e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-4fc1731db960>:465: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ],
      "source": [
        "#--------------------------------------------------------\n",
        "#       🧼 Preprocessing / Input Handling\n",
        "#--------------------------------------------------------\n",
        "\n",
        "def resize_image(image, size=(384, 384)):\n",
        "    \"\"\"\n",
        "    Safely resize uploaded image for vision models.\n",
        "    \"\"\"\n",
        "    return image.resize(size)\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"ProcessImagePipeline\")\n",
        "def process_image(image, mode=\"Generate New Meal Plan\"):\n",
        "    \"\"\"\n",
        "    Handles image > ingredients > generate meal OR search recipes based on user mode.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if image is None:\n",
        "            return \"❌ Please upload a valid image.\"\n",
        "\n",
        "        print(\"📸 Processing image...\")\n",
        "        image = resize_image(image)\n",
        "\n",
        "        ingredients, caption = vision_detector.detect_ingredients(image)\n",
        "        print(f\"✅ Detected Ingredients: {ingredients}\")\n",
        "\n",
        "        if not ingredients:\n",
        "            return \"❌ No ingredients could be detected from the image.\"\n",
        "\n",
        "        if mode == \"Generate New Meal Plan\":\n",
        "            meal_plan = generate_grounded_meal_plan(ingredients, collection, embedding_model, client)\n",
        "            meal_nutrition, nutrition_advice = generate_nutrition_facts_and_advice(meal_plan)\n",
        "            return format_output(ingredients, meal_plan, meal_nutrition, nutrition_advice)\n",
        "\n",
        "\n",
        "        elif mode == \"Search Existing Recipes\":\n",
        "            ingredient_query = \", \".join(ingredients)\n",
        "            recipes = search_recipes(ingredient_query)\n",
        "            return f\"\"\"🧺 **Detected Ingredients**:\n",
        "{ingredient_query}\n",
        "\n",
        "📚 **Top Recipes**:\n",
        "\"\"\" + \"\\n\\n\".join(recipes)\n",
        "\n",
        "        else:\n",
        "            return \"❌ Invalid mode selected.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error in process_image(): {e}\")\n",
        "        return \"❌ An unexpected error occurred.\"\n",
        "\n",
        "\n",
        "\n",
        "# === Vision Model for Ingredient Detection ===\n",
        "\n",
        "class IngredientDetector:\n",
        "    \"\"\"\n",
        "    Loads a BLIP model for ingredient detection from images.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None):\n",
        "        import torch\n",
        "        from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(self.device)\n",
        "\n",
        "    def detect_ingredients(self, image):\n",
        "        \"\"\"\n",
        "        Detects ingredients and generates a caption from the image.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n",
        "            output = self.model.generate(**inputs)\n",
        "            caption = self.processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract ingredients from caption simply (split commas)\n",
        "            ingredients_list = [x.strip() for x in caption.split(',') if x.strip()]\n",
        "            return ingredients_list, caption\n",
        "        except Exception as e:\n",
        "            print(f\"🔥 Error detecting ingredients: {e}\")\n",
        "            return [], \"Error\"\n",
        "\n",
        "\n",
        "\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"ExtractIngredientsFromAudio\")\n",
        "def extract_ingredients(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribes the audio and extracts ingredients using GPT.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"🎤 Transcribing audio from: {audio_path}\")\n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "        full_text = result[\"text\"]\n",
        "        print(f\"📝 Transcription: {full_text}\")\n",
        "\n",
        "        # Extract ingredients using GPT\n",
        "        prompt = f\"\"\"\n",
        "You're a kitchen assistant. The user said:\n",
        "\n",
        "\"{full_text}\"\n",
        "\n",
        "From this sentence, extract and return only the list of ingredients mentioned (just food items).\n",
        "Return them as a Python list. Do not include extra commentary.\n",
        "\"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You extract ingredients from text.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        extracted = response.choices[0].message.content.strip()\n",
        "        print(f\"✅ Extracted: {extracted}\")\n",
        "\n",
        "        # Evaluate string safely as list (e.g., \"['eggs', 'tomatoes']\")\n",
        "        ingredients = eval(extracted)\n",
        "        return ingredients\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error extracting ingredients: {e}\")\n",
        "        return []\n",
        "\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"ProcessAudioPipeline\")\n",
        "def process_audio(audio_path, mode=\"Generate New Meal Plan\"):\n",
        "    try:\n",
        "        if not audio_path:\n",
        "            print(\"❌ No audio path received.\")\n",
        "            return \"❌ Please upload a valid audio file.\"\n",
        "\n",
        "        print(\"🎤 Starting to process audio...\")\n",
        "        ingredients = speech_detector.extract_ingredients(audio_path)\n",
        "        print(f\"✅ Detected Ingredients: {ingredients}\")\n",
        "\n",
        "        if not ingredients:\n",
        "            print(\"❌ No ingredients detected.\")\n",
        "            return \"❌ No ingredients could be detected from the audio.\"\n",
        "\n",
        "        print(f\"🔄 Selected Mode: {mode}\")\n",
        "\n",
        "        if mode == \"Generate New Meal Plan\":\n",
        "            meal_plan = generate_grounded_meal_plan(ingredients, collection, embedding_model, client)\n",
        "            print(f\"✅ Generated Meal Plan.\")\n",
        "            meal_nutrition, nutrition_advice = generate_nutrition_facts_and_advice(meal_plan)\n",
        "            print(f\"✅ Generated Nutrition Advice.\")\n",
        "            return format_output(ingredients, meal_plan, meal_nutrition, nutrition_advice)\n",
        "\n",
        "\n",
        "        elif mode == \"Search Existing Recipes\":\n",
        "            ingredient_query = \", \".join(ingredients)\n",
        "            recipes = search_recipes(ingredient_query)\n",
        "            print(f\"✅ Retrieved {len(recipes)} Recipes.\")\n",
        "            return f\"\"\"🧺 **Detected Ingredients**:\n",
        "{ingredient_query}\n",
        "\n",
        "📚 **Top Recipes**:\n",
        "\"\"\" + \"\\n\\n\".join(recipes)\n",
        "\n",
        "        else: # elif mode == \"invalid mode\"\n",
        "            print(\"❌ Invalid mode selected.\")\n",
        "            return \"❌ Invalid mode selected.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 FULL ERROR TRACE: {e}\")\n",
        "        print(f\"The audio path type is: {type(audio_path)} and the mode is: {mode}\")\n",
        "        # return \"❌ An unexpected error occurred.\"\n",
        "        return f\"The audio path type is: {type(audio_path)} and the mode is: {mode}\"\n",
        "\n",
        "#__________________________________________________________________________________________________________________________________________________\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "#         🍳 Meal & Nutrition Generation\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "@traceable(name=\"GenerateGroundedMealPlan\")\n",
        "def generate_grounded_meal_plan(ingredients_list, collection, embedding_model, client):\n",
        "    \"\"\"\n",
        "    Uses real recipes from the vector store to generate a meal plan grounded in actual data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ingredient_query = \", \".join(ingredients_list)\n",
        "        query_embedding = embedding_model.embed_query(ingredient_query)\n",
        "\n",
        "        # Step 1: Retrieve top matching recipes from Chroma\n",
        "        results = collection.query(\n",
        "            query_embeddings=query_embedding,\n",
        "            n_results=10,\n",
        "            include=[\"documents\", \"metadatas\"]\n",
        "        )\n",
        "\n",
        "        if not results[\"documents\"] or not results[\"documents\"][0]:\n",
        "            return \"❌ No relevant recipes found in dataset.\"\n",
        "\n",
        "        retrieved_docs = results[\"documents\"][0]\n",
        "\n",
        "        # Step 2: Prepare prompt using real recipes\n",
        "        context = \"\\n\\n\".join([f\"Recipe: {doc}\" for doc in retrieved_docs])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are an AI chef. Based on the following real recipes:\n",
        "\n",
        "{context}\n",
        "\n",
        "Create a balanced meal plan for today:\n",
        "- One Breakfast meal\n",
        "- One Lunch meal\n",
        "- One Dinner meal\n",
        "\n",
        "Only use ingredients and meal ideas inspired by these recipes.\n",
        "Format:\n",
        "Breakfast: ...\n",
        "Lunch: ...\n",
        "Dinner: ...\n",
        "\"\"\"\n",
        "\n",
        "        # Step 3: Send to GPT\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful culinary AI assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error in generate_grounded_meal_plan(): {e}\")\n",
        "        return \"❌ Failed to generate meal plan from dataset.\"\n",
        "\n",
        "\n",
        "#_______________________\n",
        "\n",
        "def lookup_nutrition(meal_plan, recipes_df):\n",
        "    \"\"\"\n",
        "    Tries to match each meal in the plan to a recipe and extracts nutrition info.\n",
        "    Returns a summary string.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for line in meal_plan.lower().splitlines():\n",
        "        if \":\" not in line:  # Skip lines without clear meal info\n",
        "            continue\n",
        "\n",
        "        label, meal = line.split(\":\", 1)\n",
        "        meal = meal.strip()\n",
        "\n",
        "        match = recipes_df[recipes_df[\"name\"].str.contains(meal, case=False, na=False)]\n",
        "        if not match.empty:\n",
        "            nutrition = match.iloc[0][\"nutrition\"]\n",
        "            try:\n",
        "                nutrition_data = ast.literal_eval(nutrition)\n",
        "                nutrition_summary = f\"Calories: {nutrition_data[0]} kcal, Fat: {nutrition_data[1]}g, Carbs: {nutrition_data[2]}g, Protein: {nutrition_data[3]}g\"\n",
        "            except Exception:\n",
        "                nutrition_summary = nutrition  # fallback if parsing fails\n",
        "        else:\n",
        "            nutrition_summary = \"Nutrition data not found.\"\n",
        "\n",
        "        results.append(f\"{label.capitalize()}: {meal} ➤ {nutrition_summary}\")\n",
        "\n",
        "    return \"\\n\".join(results)\n",
        "\n",
        "#_______________________\n",
        "\n",
        "def get_nutrition_info(meal_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Look up nutrition info for a given meal name using the dataset.\n",
        "    \"\"\"\n",
        "    match = recipes_df[recipes_df[\"name\"].str.contains(meal_name, case=False, na=False)]\n",
        "    if not match.empty:\n",
        "        nutrition = match.iloc[0][\"nutrition\"]\n",
        "        try:\n",
        "            nutrition_data = ast.literal_eval(nutrition)\n",
        "            return f\"Calories: {nutrition_data[0]} kcal, Fat: {nutrition_data[1]}g, Carbs: {nutrition_data[2]}g, Protein: {nutrition_data[3]}g\"\n",
        "        except:\n",
        "            return nutrition  # fallback\n",
        "    else:\n",
        "        return \"Nutrition data not found.\"\n",
        "\n",
        "from langchain.tools import Tool\n",
        "\n",
        "nutrition_tool = Tool(\n",
        "    name=\"get_nutrition_info\",\n",
        "    func=get_nutrition_info,\n",
        "    description=\"Use this tool to fetch nutrition data for a meal name. Input is the meal name as a string.\"\n",
        ")\n",
        "#_______________________\n",
        "\n",
        "def get_meal_nutrition_breakdown(meal_plan, df):\n",
        "    \"\"\"\n",
        "    Returns a dict of meal -> nutrition string using combined_df.\n",
        "    \"\"\"\n",
        "    meal_nutrition = {}\n",
        "    for line in meal_plan.lower().splitlines():\n",
        "        if \":\" not in line:\n",
        "            continue\n",
        "        label, meal = line.split(\":\", 1)\n",
        "        meal = meal.strip()\n",
        "\n",
        "        match = df[df[\"name\"].str.contains(meal, case=False, na=False)]\n",
        "        if not match.empty:\n",
        "            nutrition = match.iloc[0][\"nutrition\"]\n",
        "            try:\n",
        "                data = ast.literal_eval(nutrition)\n",
        "                summary = f\"Calories: {data[0]} kcal, Fat: {data[1]}g, Carbs: {data[2]}g, Protein: {data[3]}g\"\n",
        "            except:\n",
        "                summary = nutrition\n",
        "        else:\n",
        "            summary = \"Nutrition not found.\"\n",
        "\n",
        "        meal_nutrition[label.capitalize()] = f\"{meal} ➤ {summary}\"\n",
        "\n",
        "    return meal_nutrition\n",
        "\n",
        "\n",
        "\n",
        "@traceable(name=\"GenerateNutritionAdvice\")\n",
        "def generate_nutrition_facts_and_advice(meal_plan):\n",
        "    try:\n",
        "        print(\"🧪 Meal Plan Input:\")\n",
        "        print(meal_plan)\n",
        "\n",
        "        # ✅ Step 1: Lookup real nutrition data\n",
        "        meal_nutrition = get_meal_nutrition_breakdown(meal_plan, combined_df)\n",
        "        grounded_nutrition = \"\\n\".join(meal_nutrition.values())\n",
        "        print(\"📊 Grounded Nutrition Extracted:\")\n",
        "        print(grounded_nutrition)\n",
        "\n",
        "        # ✅ Step 2: Prompt GPT\n",
        "        prompt = f\"\"\"\n",
        "You are a nutritionist.\n",
        "\n",
        "Here is the meal plan for today:\n",
        "{meal_plan}\n",
        "\n",
        "And here are the grounded nutrition facts for each meal:\n",
        "{grounded_nutrition}\n",
        "\n",
        "Now:\n",
        "- Give a short nutritional summary for the day\n",
        "- Suggest 2 health tips to improve the meal plan\n",
        "\"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a licensed dietitian.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        print(\"✅ GPT Response Received.\")\n",
        "        return meal_nutrition, response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 ERROR in generate_nutrition_facts_and_advice(): {e}\")\n",
        "        return {}, \"❌ Error generating nutrition advice.\"\n",
        "\n",
        "\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"FormatFinalOutput\")\n",
        "def format_output(ingredients_list, meal_plan, meal_nutrition, nutrition_advice):\n",
        "    try:\n",
        "        nutrition_details = \"\\n\".join([f\"{k}: {v}\" for k, v in meal_nutrition.items()])\n",
        "\n",
        "        final_text = f\"\"\"🧺 **Detected Ingredients**:\n",
        "{', '.join(ingredients_list)}\n",
        "\n",
        "🍽️ **Meal Plan**:\n",
        "{meal_plan}\n",
        "\n",
        "📊 **Nutrition Per Meal**:\n",
        "{nutrition_details}\n",
        "\n",
        "🧪 **Nutrition Summary & Advice**:\n",
        "{nutrition_advice}\n",
        "\"\"\"\n",
        "        return final_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"❌ Error formatting final output.\"\n",
        "\n",
        "\n",
        "#________________________________________________________________________________________________________________________________________\n",
        "\n",
        "#------------------------------------------------------------\n",
        "#            🔍 Embedding + Retrieval\n",
        "#------------------------------------------------------------\n",
        "\n",
        "@traceable(name=\"EmbedQuery\")\n",
        "def embed_query(text_query):\n",
        "    return embedding_model.embed_query(text_query)\n",
        "\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"SearchRecipes\")\n",
        "def search_recipes(ingredient_query, top_k=5):\n",
        "    query_embedding = embed_query(ingredient_query)\n",
        "    results = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=top_k,\n",
        "        include=[\"documents\"]\n",
        "    )\n",
        "    return [f\"• {r}\" for r in results[\"documents\"][0]]\n",
        "\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"GetRecipeContext\")\n",
        "def get_recipe_context(ingredients_list, top_k=3):\n",
        "    ingredient_query = \", \".join(ingredients_list)\n",
        "    query_embedding = embedding_model.embed_query(ingredient_query)\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=top_k,\n",
        "        include=[\"documents\", \"metadatas\"]\n",
        "    )\n",
        "\n",
        "    context_lines = []\n",
        "    for i, doc in enumerate(results[\"documents\"][0]):\n",
        "        meta = results[\"metadatas\"][0][i]\n",
        "        cuisine = meta.get(\"cuisine_path\", \"Unknown\")\n",
        "        timing = meta.get(\"timing\", \"N/A\")\n",
        "        instructions = meta.get(\"instructions\", \"No directions available.\")\n",
        "        context_lines.append(\n",
        "            f\"• {doc} ({cuisine}, takes {timing})\\nDirections: {instructions}\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\\n\".join(context_lines)\n",
        "\n",
        "#____________________________________________________________________________________________________________________________________\n",
        "\n",
        "# Tool 1: Recipe Search\n",
        "recipe_tool = Tool(\n",
        "    name=\"SearchRecipe\",\n",
        "    func=search_recipes,\n",
        "    description=\"Search for recipes based on a list of ingredients.\"\n",
        ")\n",
        "\n",
        "# Tool 2: Nutrition Info Lookup\n",
        "nutrition_tool = Tool(\n",
        "    name=\"GetNutritionInfo\",\n",
        "    func=get_nutrition_info,\n",
        "    description=\"Get nutrition details for a specific meal name.\"\n",
        ")\n",
        "\n",
        "meal_plan_tool = Tool(\n",
        "    name=\"GenerateMealPlan\",\n",
        "    func=lambda ingredients: generate_grounded_meal_plan(ingredients, collection, embedding_model, client),\n",
        "    description=\"Generate a daily meal plan based on a list of ingredients using real dataset recipes.\"\n",
        ")\n",
        "\n",
        "#___________________\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=[recipe_tool, nutrition_tool, meal_plan_tool],\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "#____________________________________________________________________________________________________________________________________\n",
        "\n",
        "\n",
        "#--------------------------------------------------------\n",
        "#              💬 Assistant Chatbot\n",
        "#--------------------------------------------------------\n",
        "\n",
        "def get_context_from_query(query):\n",
        "    try:\n",
        "        results = collection.query(\n",
        "            query_embeddings=embedding_model.embed_query(query),\n",
        "            n_results=3,\n",
        "            include=[\"documents\", \"metadatas\"]\n",
        "        )\n",
        "        contexts = []\n",
        "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
        "            ctx = f\"{doc} | Cuisine: {meta.get('cuisine_path', 'Unknown')}, Time: {meta.get('timing', 'N/A')}, Instructions: {meta.get('instructions', 'N/A')}\"\n",
        "            contexts.append(ctx)\n",
        "        return \"\\n\\n\".join(contexts)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in get_context_from_query(): {e}\")\n",
        "        return \"No relevant recipes found.\"\n",
        "\n",
        "\n",
        "# @traceable(name=\"SmartNutriChefChat\")\n",
        "# def handle_chat(user_message, session_id, chat_history, last_dish):\n",
        "#     try:\n",
        "#         if not user_message.strip():\n",
        "#             return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#         print(f\"🧠 User input: {user_message}\")\n",
        "\n",
        "#         # Handle \"how to make it\" follow-up\n",
        "#         # if \"how to make\" in user_message.lower() and \"it\" in user_message.lower() and last_dish:\n",
        "#         #     user_message = f\"How do I make {last_dish}?\"\n",
        "#         # Simple coreference resolution\n",
        "#         if any(pronoun in user_message.lower() for pronoun in [\"how to make it\", \"how to cook it\", \"how to prepare it\", \"how do i make it\"]) and last_dish:\n",
        "#             user_message = f\"How do I make {last_dish}?\"\n",
        "#         elif any(pronoun in user_message.lower() for pronoun in [\"is it healthy\", \"is it good\", \"what does it contain\"]) and last_dish:\n",
        "#             user_message = f\"Is {last_dish} healthy?\"\n",
        "\n",
        "\n",
        "#         # Get recipe context for retrieval\n",
        "#         recipe_context = get_context_from_query(user_message)\n",
        "\n",
        "#         # Generate response using memory-aware chain\n",
        "#         response = memory_chain.invoke(\n",
        "#             {\n",
        "#             \"input\": user_message,\n",
        "#             \"recipe_context\": recipe_context,\n",
        "#             \"last_dish\": last_dish or \"none\"\n",
        "#             },\n",
        "#             config={\"configurable\": {\"session_id\": session_id}}\n",
        "# )\n",
        "\n",
        "\n",
        "#         # Extract new potential dish name\n",
        "#         import re\n",
        "#         match = re.search(r\"(?:want|try|make|suggest(?: me)?|recipe for|dish like)\\s+([a-zA-Z\\s]+)\", user_message.lower())\n",
        "#         if match:\n",
        "#             candidate = match.group(1).strip()\n",
        "#             if len(candidate.split()) <= 6:  # avoid noisy or vague phrases\n",
        "#                 last_dish = candidate\n",
        "\n",
        "#         # Update chat\n",
        "#         chat_history.append([user_message, response.content])\n",
        "#         return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"🔥 Error in handle_chat: {e}\")\n",
        "#         return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#________________________________________________________________________________________________________________________________\n",
        "\n",
        "#----------------------------------------------------\n",
        "#          🎥 YouTube QA Pipeline\n",
        "#----------------------------------------------------\n",
        "\n",
        "@traceable(name=\"DownloadYouTubeAudio\")\n",
        "def download_youtube_audio(youtube_url, output_path=\"./downloads\"):\n",
        "    \"\"\"\n",
        "    Downloads audio from a YouTube video and saves as an MP3 file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': f'{output_path}/%(title)s.%(ext)s',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "            'quiet': False\n",
        "        }\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(youtube_url, download=True)\n",
        "            filename = ydl.prepare_filename(info)\n",
        "            filename = filename.replace('.webm', '.mp3').replace('.m4a', '.mp3')\n",
        "\n",
        "        print(f\"✅ Downloaded audio file: {filename}\")\n",
        "        return filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error downloading YouTube audio: {e}\")\n",
        "        return None\n",
        "\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"TranscribeYouTubeAudio\")\n",
        "def transcribe_audio(audio_file_path):\n",
        "    \"\"\"\n",
        "    Transcribes an audio file into text using Whisper.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"🎤 Transcribing: {audio_file_path}\")\n",
        "        result = whisper_model.transcribe(audio_file_path)\n",
        "        transcript_text = result[\"text\"]\n",
        "        print(\"✅ Transcription finished.\")\n",
        "        return transcript_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error during transcription: {e}\")\n",
        "        return \"❌ Error transcribing audio.\"\n",
        "\n",
        "#_______________________\n",
        "\n",
        "@traceable(name=\"ChunkTranscript\")\n",
        "def chunk_transcript(transcript_text, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Splits a large transcript text into smaller chunks based on sentence boundaries.\n",
        "    \"\"\"\n",
        "    print(\"✂️ Chunking the transcript...\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in transcript_text.split('. '):\n",
        "        if len(current_chunk) + len(sentence) <= chunk_size:\n",
        "            current_chunk += sentence + \". \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "\n",
        "    # Add the last chunk if any remains\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    print(f\"✅ Total chunks created: {len(chunks)}\")\n",
        "    return chunks\n",
        "\n",
        "# === Embedding Chunks into ChromaDB ===\n",
        "\n",
        "@traceable(name=\"EmbedAndStoreChunks\")\n",
        "def embed_and_store_chunks(chunks, youtube_url):\n",
        "    \"\"\"\n",
        "    Embeds the transcript chunks and stores them into ChromaDB.\n",
        "    \"\"\"\n",
        "    print(\"🧠 Embedding chunks and saving to ChromaDB...\")\n",
        "\n",
        "    # Create IDs for each chunk\n",
        "    ids = [f\"{youtube_url}_chunk_{i}\" for i in range(len(chunks))]\n",
        "\n",
        "    # Embed all chunks\n",
        "    # embeddings = embedding_model.encode(chunks).tolist()\n",
        "    embeddings = embedding_model.embed_documents(chunks)\n",
        "\n",
        "    # Store into Chroma\n",
        "    video_collection.add(\n",
        "       documents=chunks,\n",
        "       embeddings=embeddings,\n",
        "       ids=ids,\n",
        "       metadatas=[{\"source\": youtube_url}] * len(chunks)\n",
        ")\n",
        "\n",
        "    print(f\"✅ {len(chunks)} chunks embedded and saved successfully.\")\n",
        "\n",
        "\n",
        "# === Create Retriever Function ===\n",
        "\n",
        "def create_retriever():\n",
        "    \"\"\"\n",
        "    Create a retriever for the 'youtube_videos' collection.\n",
        "    \"\"\"\n",
        "    db = Chroma(\n",
        "        client=chroma_client,\n",
        "        collection_name=\"youtube_videos\",\n",
        "        embedding_function=embedding_model\n",
        "    )\n",
        "    retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
        "    return retriever\n",
        "\n",
        "# === Create QA Chain Function ===\n",
        "\n",
        "def create_qa_chain():\n",
        "    \"\"\"\n",
        "    Create a RetrievalQA chain using Chroma retriever and OpenAI LLM.\n",
        "    \"\"\"\n",
        "    retriever = create_retriever()\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "\n",
        "# === Handle YouTube Download, Transcription, Chunking, Embedding ===\n",
        "\n",
        "@traceable(name=\"HandleYouTubeDownload\")\n",
        "def handle_youtube_download(youtube_url):\n",
        "    if not youtube_url:\n",
        "        return \"❌ Please enter a valid YouTube link.\"\n",
        "\n",
        "    audio_file = download_youtube_audio(youtube_url)\n",
        "    if audio_file:\n",
        "        transcript = transcribe_audio(audio_file)\n",
        "\n",
        "        chunks = chunk_transcript(transcript)\n",
        "\n",
        "        # === NEW: Embed and Store\n",
        "        embed_and_store_chunks(chunks, youtube_url)\n",
        "\n",
        "        global qa_chain\n",
        "        qa_chain = create_qa_chain()\n",
        "\n",
        "        # === Preview first 5 chunks\n",
        "        preview_text = \"\\n\\n\".join(chunks[:5])\n",
        "\n",
        "        return preview_text\n",
        "\n",
        "    else:\n",
        "        return \"❌ Failed to download audio. Please check the link.\"\n",
        "\n",
        "# === QA over Video Function ===\n",
        "\n",
        "@traceable(name=\"AnswerVideoQuestion\")\n",
        "def answer_video_question(user_question):\n",
        "    \"\"\"\n",
        "    Answers a user question about the uploaded YouTube video.\n",
        "    \"\"\"\n",
        "    if not user_question:\n",
        "        return \"❌ Please enter a question.\"\n",
        "\n",
        "    try:\n",
        "        print(\"🔍 Asking video question:\", user_question)\n",
        "\n",
        "        if qa_chain is None:\n",
        "            print(\"❌ qa_chain is None\")\n",
        "            return \"❌ QA system not initialized.\"\n",
        "\n",
        "        result = qa_chain(user_question)\n",
        "        print(\"✅ Raw result from QA chain:\", result)\n",
        "\n",
        "        if \"result\" not in result:\n",
        "            print(\"❌ 'result' key missing in QA output.\")\n",
        "            return \"❌ Unexpected response format.\"\n",
        "\n",
        "        return result[\"result\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error during video QA: {e}\")\n",
        "        return f\"❌ Error: {e}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KFD1sbeKS1W"
      },
      "source": [
        "### ✅ Evaluation / Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bviYNzuvQiH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37eb325-0f4b-444c-d6af-7e1739cb54ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-cf9925e40b41>:19: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
            "<ipython-input-20-cf9925e40b41>:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(agent.run(\"What are the nutrition facts for Tomato Omelette?\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Tool Output:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use the get_nutrition_info tool to fetch the nutrition data for Tomato Omelette.\n",
            "Action: get_nutrition_info\n",
            "Action Input: \"Tomato Omelette\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNutrition data not found.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe nutrition data for Tomato Omelette is not available.\n",
            "Final Answer: Nutrition data not found.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Nutrition data not found.\n"
          ]
        }
      ],
      "source": [
        "# from langchain.evaluation import run_on_dataset\n",
        "# from langsmith.evaluation import Example, LangChainStringEvaluator\n",
        "from langchain.evaluation.loading import load_dataset\n",
        "from langchain.evaluation.schema import StringEvaluator\n",
        "\n",
        "# === Define AI Judge ===\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.evaluation.criteria.eval_chain import CriteriaEvalChain\n",
        "\n",
        "# === 1. Define the Nutrition Tool ===\n",
        "nutrition_tool = Tool(\n",
        "    name=\"get_nutrition_info\",\n",
        "    func=get_nutrition_info,\n",
        "    description=\"Use this tool to fetch nutrition data for a meal name. Input is the meal name as a string.\"\n",
        ")\n",
        "\n",
        "# === 2. Initialize an LLM for both tools and evaluation ===\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# === 3. Create a LangChain Agent that can use your tool ===\n",
        "agent = initialize_agent(\n",
        "    tools=[nutrition_tool],\n",
        "    llm=llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# === 4. Run a test question with the tool ===\n",
        "print(\"🔍 Tool Output:\")\n",
        "print(agent.run(\"What are the nutrition facts for Tomato Omelette?\"))\n",
        "\n",
        "# === 5. Define your LangChain Evaluator (LangSmith compatible) ===\n",
        "meal_plan_evaluator = CriteriaEvalChain.from_llm(\n",
        "    llm=llm,\n",
        "    criteria={\n",
        "        \"relevance\": \"Does the meal plan match the detected ingredients?\",\n",
        "        \"completeness\": \"Does it cover all 3 main meals (breakfast, lunch, dinner)?\",\n",
        "        \"creativity\": \"Is the meal plan creative and varied?\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# === 6. Evaluation Function ===\n",
        "def evaluate_output(ingredients_list, meal_plan_text):\n",
        "    try:\n",
        "        # Format input for evaluator\n",
        "        inputs = {\"ingredients\": \", \".join(ingredients_list)}\n",
        "        prediction = {\"meal_plan\": meal_plan_text}\n",
        "\n",
        "        # Run evaluation\n",
        "        result = meal_plan_evaluator.evaluate_strings(\n",
        "            input=inputs,\n",
        "            prediction=prediction\n",
        "        )\n",
        "\n",
        "        print(\"🧠 Judgment:\", result[\"reasoning\"])\n",
        "        print(\"📊 Score:\", result[\"score\"])\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error during evaluation: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89NzlVt_Qnba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec7ef39-cff1-441a-c89c-17520b08c27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Judgment: Relevance: The meal plan includes a Cheese Omelette, Tomato Sandwich, and Grilled Cheese with Tomato Soup, which all contain cheese and tomatoes, two of the detected ingredients. Therefore, the meal plan is relevant to the detected ingredients.\n",
            "\n",
            "Completeness: The meal plan covers all 3 main meals - breakfast, lunch, and dinner. Each meal is specified in the plan, so it is complete in that sense.\n",
            "\n",
            "Creativity: The meal plan shows creativity by incorporating the detected ingredients in different ways for each meal. The variety of dishes such as omelette, sandwich, and soup with grilled cheese shows creativity in meal planning.\n",
            "\n",
            "Y\n",
            "📊 Score: 1\n"
          ]
        }
      ],
      "source": [
        "# === Example Evaluation Run ===\n",
        "ingredients_used = [\"cheese\", \"bread\", \"tomatoes\"]\n",
        "\n",
        "meal_plan_generated = \"\"\"\n",
        "Breakfast: Cheese Omelette\n",
        "Lunch: Tomato Sandwich\n",
        "Dinner: Grilled Cheese with Tomato Soup\n",
        "\"\"\"\n",
        "\n",
        "# Run evaluation\n",
        "result = evaluate_output(ingredients_used, meal_plan_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svB217mrXCIF"
      },
      "source": [
        "## 📍 Step 3: App Layout\n",
        "The app will have two tabs:\n",
        "- 📷 Upload Image (Vision)\n",
        "- 🎤 Record Voice (Speech-to-Text)\n",
        "Each will detect ingredients, retrieve recipes, plan meals, and show nutrition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9A5kPniWzQs"
      },
      "outputs": [],
      "source": [
        "# custom_css = \"\"\"\n",
        "# body {\n",
        "#     background: linear-gradient(to right, #fdf6e3, #fefcea);\n",
        "#     font-family: 'Segoe UI', sans-serif;\n",
        "# }\n",
        "\n",
        "# .gr-button {\n",
        "#     border-radius: 12px !important;\n",
        "#     font-weight: bold !important;\n",
        "#     padding: 8px 16px !important;\n",
        "#     box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);\n",
        "# }\n",
        "\n",
        "# .gr-button:hover {\n",
        "#     background-color: #f4e4c1 !important;\n",
        "#     color: #333 !important;\n",
        "# }\n",
        "\n",
        "# .gr-textbox textarea, .gr-radio label {\n",
        "#     border-radius: 8px !important;\n",
        "#     background-color: #fffefc !important;\n",
        "# }\n",
        "\n",
        "# .gr-chatbot {\n",
        "#     background-color: #fffdfa !important;\n",
        "#     border-radius: 10px;\n",
        "#     border: 1px solid #f0e9dc;\n",
        "# }\n",
        "# \"\"\"\n",
        "\n",
        "# with gr.Blocks(css=custom_css) as demo:\n",
        "\n",
        "#     chat_history = gr.State([])\n",
        "#     session_id = gr.State(get_session_id())\n",
        "#     last_dish_state = gr.State(\"\")\n",
        "\n",
        "\n",
        "#     gr.Markdown(\"\"\"\n",
        "#     <h1 style='text-align: center; color: #5c4b2c;'>🍽️ NutriChef AI</h1>\n",
        "#     <p style='text-align: center; font-size: 17px;'>Your Friendly AI Chef — Generate meal plans, extract nutrition, and answer food questions from image, audio, or video! 🎉</p>\n",
        "#     \"\"\")\n",
        "\n",
        "#     with gr.Tab(\"📷 Upload Fridge Image\"):\n",
        "#         with gr.Row():\n",
        "#             with gr.Column():\n",
        "#                 image_input = gr.Image(type=\"pil\", label=\"Upload Fridge Image\")\n",
        "#                 meal_mode = gr.Radio(\n",
        "#                     choices=[\"Generate New Meal Plan\", \"Search Existing Recipes\"],\n",
        "#                     label=\"Choose Task\",\n",
        "#                     value=\"Generate New Meal Plan\"\n",
        "#                 )\n",
        "#                 submit_image = gr.Button(\"🍳 Find My Meals!\", elem_id=\"btn-image\")\n",
        "\n",
        "#             with gr.Column():\n",
        "#                 output_text_image = gr.Textbox(label=\"NutriChef Output\", lines=10)\n",
        "\n",
        "#         submit_image.click(\n",
        "#             process_image,\n",
        "#             inputs=[image_input, meal_mode],\n",
        "#             outputs=[output_text_image]\n",
        "#         )\n",
        "\n",
        "#     with gr.Tab(\"🎤 Upload Ingredients Audio\"):\n",
        "#         with gr.Row():\n",
        "#             with gr.Column():\n",
        "#                 audio_input = gr.Audio(sources=[\"upload\"], type=\"filepath\", label=\"Upload Audio (e.g. 'I have eggs and cheese')\")\n",
        "#                 meal_mode_audio = gr.Radio(\n",
        "#                     choices=[\"Generate New Meal Plan\", \"Search Existing Recipes\"],\n",
        "#                     label=\"Choose Task\",\n",
        "#                     value=\"Generate New Meal Plan\"\n",
        "#                 )\n",
        "#                 submit_audio = gr.Button(\"🎤 Find My Meals!\", elem_id=\"btn-audio\")\n",
        "\n",
        "#             with gr.Column():\n",
        "#                 output_text_audio = gr.Textbox(label=\"NutriChef Output\", lines=10)\n",
        "\n",
        "#         submit_audio.click(\n",
        "#             process_audio,\n",
        "#             inputs=[audio_input, meal_mode_audio],\n",
        "#             outputs=[output_text_audio]\n",
        "#         )\n",
        "\n",
        "#     with gr.Tab(\"📹 Upload Cooking Video\"):\n",
        "#         with gr.Row():\n",
        "#             with gr.Column():\n",
        "#                 youtube_link = gr.Textbox(\n",
        "#                     label=\"Paste YouTube Link\",\n",
        "#                     placeholder=\"https://www.youtube.com/watch?v=example\"\n",
        "#                 )\n",
        "#                 download_button = gr.Button(\"🎥 Analyze Video\", elem_id=\"btn-video\")\n",
        "\n",
        "#                 user_question = gr.Textbox(\n",
        "#                     label=\"Ask a Question About the Video\",\n",
        "#                     placeholder=\"e.g. What did the chef cook for dinner?\"\n",
        "#                 )\n",
        "#                 ask_button = gr.Button(\"Ask\", elem_id=\"btn-ask\")\n",
        "\n",
        "#             with gr.Column():\n",
        "#                 transcript_output = gr.Textbox(\n",
        "#                     label=\"📜 Transcript Preview\",\n",
        "#                     placeholder=\"Transcript will appear here...\",\n",
        "#                     lines=10\n",
        "#                 )\n",
        "#                 answer_output = gr.Textbox(\n",
        "#                     label=\"Answer\",\n",
        "#                     placeholder=\"AI response will appear here...\",\n",
        "#                     lines=5\n",
        "#                 )\n",
        "\n",
        "#         download_button.click(\n",
        "#             handle_youtube_download,\n",
        "#             inputs=[youtube_link],\n",
        "#             outputs=[transcript_output]\n",
        "#         )\n",
        "\n",
        "#         ask_button.click(\n",
        "#             answer_video_question,\n",
        "#             inputs=[user_question],\n",
        "#             outputs=[answer_output]\n",
        "#         )\n",
        "\n",
        "#     ingredients_state = gr.State()\n",
        "#     meal_plan_state = gr.State()\n",
        "\n",
        "#     with gr.Accordion(\"💬 NutriChef AI Assistant Chat\", open=False):\n",
        "#         chatbot = gr.Chatbot(label=\"NutriChef Chat\", value=[])\n",
        "#         user_message = gr.Textbox(label=\"Your Message\", placeholder=\"Ask me anything...\")\n",
        "#         send_button = gr.Button(\"💬 Send\")\n",
        "\n",
        "#         def chat_with_memory(user_message, session_id, chat_history, last_dish):\n",
        "#             try:\n",
        "#                 user_message = user_message.strip()\n",
        "#                 if not user_message:\n",
        "#                    return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#                 original_message = user_message  # Keep for dish detection\n",
        "\n",
        "#                 # Handle vague follow-ups referring to a previous dish\n",
        "#                 vague_phrases = [\n",
        "#                     \"how to make it\", \"how do i make it\",\n",
        "#                     \"is it healthy\", \"how much time\", \"can i add\", \"should i use\"\n",
        "#         ]\n",
        "#                 if any(phrase in user_message.lower() for phrase in vague_phrases) and last_dish:\n",
        "#                   llm_input = f\"{user_message} (referring to {last_dish})\"\n",
        "#                 else:\n",
        "#                   llm_input = user_message\n",
        "\n",
        "#                 # Retrieve context from Chroma\n",
        "#                 recipe_context = get_context_from_query(user_message)\n",
        "\n",
        "#                 # Generate response using memory\n",
        "#                 response = memory_chain.invoke(\n",
        "#                    {\"input\": llm_input, \"recipe_context\": recipe_context},\n",
        "#                    config={\"configurable\": {\"session_id\": session_id}}\n",
        "#         )\n",
        "\n",
        "#                 # Detect if a new dish is mentioned and store it\n",
        "#                 new_dish = detect_dish_with_llm(original_message)\n",
        "#                 if new_dish.lower() == \"none\":\n",
        "#                    new_dish = extract_dish_name(original_message)\n",
        "#                 if new_dish and new_dish.lower() != \"none\":\n",
        "#                    last_dish = new_dish\n",
        "\n",
        "#                 # Update chat UI\n",
        "#                 chat_history.append([user_message, response.content])\n",
        "#                 return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#             except Exception as e:\n",
        "#               print(f\"🔥 Error in chat_with_memory(): {e}\")\n",
        "#               return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "\n",
        "#         send_button.click(\n",
        "#           chat_with_memory,\n",
        "#           inputs=[user_message, session_id, chat_history, last_dish_state],\n",
        "#           outputs=[chatbot, user_message, last_dish_state]\n",
        "# )\n",
        "\n",
        "\n",
        "# demo.launch() # debug=True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom_css = \"\"\"\n",
        "# body {\n",
        "#     background: linear-gradient(to right, #fdf6e3, #fefcea);\n",
        "#     font-family: 'Segoe UI', sans-serif;\n",
        "# }\n",
        "\n",
        "# .gr-button {\n",
        "#     border-radius: 12px !important;\n",
        "#     font-weight: bold !important;\n",
        "#     padding: 8px 16px !important;\n",
        "#     box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);\n",
        "# }\n",
        "\n",
        "# .gr-button:hover {\n",
        "#     background-color: #f4e4c1 !important;\n",
        "#     color: #333 !important;\n",
        "# }\n",
        "\n",
        "# .gr-textbox textarea, .gr-radio label {\n",
        "#     border-radius: 8px !important;\n",
        "#     background-color: #fffefc !important;\n",
        "# }\n",
        "\n",
        "# .gr-chatbot {\n",
        "#     background-color: #fffdfa !important;\n",
        "#     border-radius: 10px;\n",
        "#     border: 1px solid #f0e9dc;\n",
        "# }\n",
        "# \"\"\"\n",
        "\n",
        "# with gr.Blocks(css=custom_css) as demo:\n",
        "#     chat_history = gr.State([])\n",
        "#     session_id = gr.State(get_session_id())\n",
        "#     last_dish_state = gr.State(\"\")\n",
        "\n",
        "#     gr.Markdown(\"\"\"\n",
        "#     <h1 style='text-align: center; color: #5c4b2c;'>🍽️ NutriChef AI</h1>\n",
        "#     <p style='text-align: center; font-size: 17px;'>Your Friendly AI Chef — Generate meal plans, extract nutrition, and answer food questions from image, audio, or video! 🎉</p>\n",
        "#     \"\"\")\n",
        "\n",
        "#     # ---------------- Tab 1: Image + Audio ----------------\n",
        "#     with gr.Tab(\"📸 Upload Image or Audio\"):\n",
        "#         with gr.Row():\n",
        "#             with gr.Column():\n",
        "#                 gr.Markdown(\"### 🖼️ Fridge Image Input\")\n",
        "#                 image_input = gr.Image(type=\"pil\", label=\"Upload Fridge Image\")\n",
        "#                 meal_mode = gr.Radio(\n",
        "#                     choices=[\"Generate New Meal Plan\", \"Search Existing Recipes\"],\n",
        "#                     label=\"Choose Task\",\n",
        "#                     value=\"Generate New Meal Plan\"\n",
        "#                 )\n",
        "#                 submit_image = gr.Button(\"🍳 Find My Meals!\", elem_id=\"btn-image\")\n",
        "#             with gr.Column():\n",
        "#                 output_text_image = gr.Textbox(label=\"NutriChef Output\", lines=10)\n",
        "\n",
        "#         submit_image.click(\n",
        "#             process_image,\n",
        "#             inputs=[image_input, meal_mode],\n",
        "#             outputs=[output_text_image]\n",
        "#         )\n",
        "\n",
        "#         with gr.Row():\n",
        "#             with gr.Column():\n",
        "#                 gr.Markdown(\"### 🎤 Ingredients Audio Input\")\n",
        "#                 audio_input = gr.Audio(sources=[\"upload\"], type=\"filepath\", label=\"Upload Audio\")\n",
        "#                 meal_mode_audio = gr.Radio(\n",
        "#                     choices=[\"Generate New Meal Plan\", \"Search Existing Recipes\"],\n",
        "#                     label=\"Choose Task\",\n",
        "#                     value=\"Generate New Meal Plan\"\n",
        "#                 )\n",
        "#                 submit_audio = gr.Button(\"🎤 Find My Meals!\", elem_id=\"btn-audio\")\n",
        "#             with gr.Column():\n",
        "#                 output_text_audio = gr.Textbox(label=\"NutriChef Output\", lines=10)\n",
        "\n",
        "#         submit_audio.click(\n",
        "#             process_audio,\n",
        "#             inputs=[audio_input, meal_mode_audio],\n",
        "#             outputs=[output_text_audio]\n",
        "#         )\n",
        "\n",
        "#     # ---------------- Tab 2: YouTube Video ----------------\n",
        "#     with gr.Tab(\"📹 Analyze Cooking Video\"):\n",
        "#         with gr.Row():\n",
        "#             with gr.Column():\n",
        "#                 youtube_link = gr.Textbox(\n",
        "#                     label=\"Paste YouTube Link\",\n",
        "#                     placeholder=\"https://www.youtube.com/watch?v=example\"\n",
        "#                 )\n",
        "#                 download_button = gr.Button(\"🎥 Analyze Video\", elem_id=\"btn-video\")\n",
        "#                 user_question = gr.Textbox(\n",
        "#                     label=\"Ask a Question About the Video\",\n",
        "#                     placeholder=\"e.g. What did the chef cook for dinner?\"\n",
        "#                 )\n",
        "#                 ask_button = gr.Button(\"Ask\", elem_id=\"btn-ask\")\n",
        "#             with gr.Column():\n",
        "#                 transcript_output = gr.Textbox(\n",
        "#                     label=\"📜 Transcript Preview\",\n",
        "#                     placeholder=\"Transcript will appear here...\",\n",
        "#                     lines=10\n",
        "#                 )\n",
        "#                 answer_output = gr.Textbox(\n",
        "#                     label=\"Answer\",\n",
        "#                     placeholder=\"AI response will appear here...\",\n",
        "#                     lines=5\n",
        "#                 )\n",
        "\n",
        "#         download_button.click(\n",
        "#             handle_youtube_download,\n",
        "#             inputs=[youtube_link],\n",
        "#             outputs=[transcript_output]\n",
        "#         )\n",
        "#         ask_button.click(\n",
        "#             answer_video_question,\n",
        "#             inputs=[user_question],\n",
        "#             outputs=[answer_output]\n",
        "#         )\n",
        "\n",
        "#     # ---------------- Tab 3: Chatbot ----------------\n",
        "#     with gr.Tab(\"💬 NutriChef Assistant\"):\n",
        "#         chatbot = gr.Chatbot(label=\"NutriChef Chat\", value=[])\n",
        "#         user_message = gr.Textbox(label=\"Your Message\", placeholder=\"Ask me anything...\")\n",
        "#         send_button = gr.Button(\"💬 Send\")\n",
        "\n",
        "#         def chat_with_memory(user_message, session_id, chat_history, last_dish):\n",
        "#             try:\n",
        "#                 user_message = user_message.strip()\n",
        "#                 if not user_message:\n",
        "#                     return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#                 original_message = user_message\n",
        "\n",
        "#                 vague_phrases = [\n",
        "#                     \"how to make it\", \"how do i make it\", \"is it healthy\",\n",
        "#                     \"how much time\", \"can i add\", \"should i use\"\n",
        "#                 ]\n",
        "#                 if any(phrase in user_message.lower() for phrase in vague_phrases) and last_dish:\n",
        "#                     llm_input = f\"{user_message} (referring to {last_dish})\"\n",
        "#                 else:\n",
        "#                     llm_input = user_message\n",
        "\n",
        "#                 recipe_context = get_context_from_query(user_message)\n",
        "\n",
        "#                 response = memory_chain.invoke(\n",
        "#                     {\"input\": llm_input, \"recipe_context\": recipe_context},\n",
        "#                     config={\"configurable\": {\"session_id\": session_id}}\n",
        "#                 )\n",
        "\n",
        "#                 new_dish = detect_dish_with_llm(original_message)\n",
        "#                 if new_dish.lower() == \"none\":\n",
        "#                     new_dish = extract_dish_name(original_message)\n",
        "#                 if new_dish and new_dish.lower() != \"none\":\n",
        "#                     last_dish = new_dish\n",
        "\n",
        "#                 chat_history.append([user_message, response.content])\n",
        "#                 return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"🔥 Error in chat_with_memory(): {e}\")\n",
        "#                 return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#         send_button.click(\n",
        "#             chat_with_memory,\n",
        "#             inputs=[user_message, session_id, chat_history, last_dish_state],\n",
        "#             outputs=[chatbot, user_message, last_dish_state]\n",
        "#         )\n",
        "\n",
        "# demo.launch()\n"
      ],
      "metadata": {
        "id": "AFdU7r5f4IyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_css = \"\"\"\n",
        "body {\n",
        "    background: linear-gradient(to right, #fdf6e3, #fefcea);\n",
        "    font-family: 'Segoe UI', sans-serif;\n",
        "}\n",
        ".gr-button {\n",
        "    border-radius: 12px !important;\n",
        "    font-weight: bold !important;\n",
        "    padding: 8px 16px !important;\n",
        "    box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        ".gr-button:hover {\n",
        "    background-color: #f4e4c1 !important;\n",
        "    color: #333 !important;\n",
        "}\n",
        ".gr-textbox textarea, .gr-radio label {\n",
        "    border-radius: 8px !important;\n",
        "    background-color: #fffefc !important;\n",
        "}\n",
        ".gr-chatbot {\n",
        "    background-color: #fffdfa !important;\n",
        "    border-radius: 10px;\n",
        "    border: 1px solid #f0e9dc;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    chat_history = gr.State([])\n",
        "    session_id = gr.State(get_session_id())\n",
        "    last_dish_state = gr.State(\"\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h1 style='text-align: center; color: #5c4b2c;'>🍽️ NutriChef AI</h1>\n",
        "    <p style='text-align: center; font-size: 17px;'>Your Friendly AI Chef — Generate meal plans, extract nutrition, and answer food questions from image, audio, or video! 🎉</p>\n",
        "    \"\"\")\n",
        "\n",
        "    # === TAB 1: Image + Audio ===\n",
        "    with gr.Tab(\"📸 Upload Image or Audio\"):\n",
        "        gr.Markdown(\"### 🖼️ Step 1: Upload Fridge Image\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload Image of Your Fridge\")\n",
        "                meal_mode = gr.Radio(\n",
        "                    choices=[\"Generate New Meal Plan\", \"Search Existing Recipes\"],\n",
        "                    label=\"Choose Task\",\n",
        "                    value=\"Generate New Meal Plan\"\n",
        "                )\n",
        "                submit_image = gr.Button(\"🍳 Analyze Image\")\n",
        "            with gr.Column():\n",
        "                output_text_image = gr.Textbox(label=\"🍽️ NutriChef Output\", lines=15)\n",
        "\n",
        "        submit_image.click(\n",
        "            process_image,\n",
        "            inputs=[image_input, meal_mode],\n",
        "            outputs=[output_text_image]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"---\")  # Divider\n",
        "\n",
        "        gr.Markdown(\"### 🎤 Step 2: Upload Ingredients as Audio\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                audio_input = gr.Audio(sources=[\"upload\"], type=\"filepath\", label=\"Upload Audio (e.g. 'I have eggs and cheese')\")\n",
        "                meal_mode_audio = gr.Radio(\n",
        "                    choices=[\"Generate New Meal Plan\", \"Search Existing Recipes\"],\n",
        "                    label=\"Choose Task\",\n",
        "                    value=\"Generate New Meal Plan\"\n",
        "                )\n",
        "                submit_audio = gr.Button(\"🎤 Analyze Audio\")\n",
        "            with gr.Column():\n",
        "                output_text_audio = gr.Textbox(label=\"🍽️ NutriChef Output\", lines=15)\n",
        "\n",
        "        submit_audio.click(\n",
        "            process_audio,\n",
        "            inputs=[audio_input, meal_mode_audio],\n",
        "            outputs=[output_text_audio]\n",
        "        )\n",
        "\n",
        "    # === TAB 2: YouTube Video ===\n",
        "    with gr.Tab(\"📹 YouTube Cooking Video\"):\n",
        "        gr.Markdown(\"### 🎬 Analyze Cooking Video and Ask Questions\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                youtube_link = gr.Textbox(\n",
        "                    label=\"Paste YouTube Link\",\n",
        "                    placeholder=\"https://www.youtube.com/watch?v=example\"\n",
        "                )\n",
        "                download_button = gr.Button(\"🎥 Analyze Video\")\n",
        "\n",
        "                user_question = gr.Textbox(\n",
        "                    label=\"Ask a Question About the Video\",\n",
        "                    placeholder=\"e.g. What did the chef cook for dinner?\"\n",
        "                )\n",
        "                ask_button = gr.Button(\"❓ Ask\")\n",
        "\n",
        "            with gr.Column():\n",
        "                transcript_output = gr.Textbox(label=\"📜 Transcript Preview\", lines=10)\n",
        "                answer_output = gr.Textbox(label=\"💬 Answer\", lines=5)\n",
        "\n",
        "        download_button.click(\n",
        "            handle_youtube_download,\n",
        "            inputs=[youtube_link],\n",
        "            outputs=[transcript_output]\n",
        "        )\n",
        "\n",
        "        ask_button.click(\n",
        "            answer_video_question,\n",
        "            inputs=[user_question],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "    # === TAB 3: Assistant Chatbot ===\n",
        "    with gr.Tab(\"💬 AI Chat Assistant\"):\n",
        "        gr.Markdown(\"### 👨‍🍳 Chat with NutriChef AI\")\n",
        "\n",
        "        chatbot = gr.Chatbot(label=\"NutriChef Chat\", value=[])\n",
        "        user_message = gr.Textbox(label=\"Your Message\", placeholder=\"Ask me anything...\")\n",
        "        send_button = gr.Button(\"💬 Send\")\n",
        "\n",
        "        def chat_with_memory(user_message, session_id, chat_history, last_dish):\n",
        "            try:\n",
        "                user_message = user_message.strip()\n",
        "                if not user_message:\n",
        "                   return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "                original_message = user_message\n",
        "\n",
        "                # ✅ Handle affirmatives like \"yes\", \"sure\" if last_dish exists\n",
        "                affirmatives = [\"yes\", \"sure\", \"okay\", \"please\", \"go ahead\"]\n",
        "                if user_message.lower().strip() in affirmatives and last_dish:\n",
        "                   llm_input = f\"Yes, I want the recipe for {last_dish}\"\n",
        "\n",
        "                # 🔁 Otherwise, handle vague follow-up phrases\n",
        "                else:\n",
        "                   vague_phrases = [\n",
        "                      \"how to make it\", \"how do i make it\",\n",
        "                      \"how to cook it\", \"how to prepare it\",\n",
        "                      \"is it healthy\", \"is it good\", \"what does it contain\",\n",
        "                      \"how much time\", \"can i add\", \"should i use\"\n",
        "                   ]\n",
        "                   if any(p in user_message.lower() for p in vague_phrases) and last_dish:\n",
        "                      llm_input = f\"{user_message} (referring to {last_dish})\"\n",
        "                   else:\n",
        "                      llm_input = user_message\n",
        "\n",
        "                # 🔍 Get real recipe context from ChromaDB\n",
        "                recipe_context = get_context_from_query(user_message)\n",
        "\n",
        "                # 🧠 Call memory-aware chain with resolved input\n",
        "                response = memory_chain.invoke(\n",
        "                  {\n",
        "                    \"input\": llm_input,\n",
        "                    \"recipe_context\": recipe_context,\n",
        "                    \"last_dish\": last_dish or \"none\"\n",
        "                  },\n",
        "                  config={\"configurable\": {\"session_id\": session_id}}\n",
        "        )\n",
        "\n",
        "                # 🍛 Detect new dish from message\n",
        "                new_dish = detect_dish_with_llm(original_message)\n",
        "                if new_dish.lower() == \"none\":\n",
        "                   new_dish = extract_dish_name(original_message)\n",
        "                if new_dish and new_dish.lower() != \"none\":\n",
        "                   print(f\"🧠 Detected Dish: {new_dish}\")\n",
        "                   last_dish = new_dish\n",
        "                else:\n",
        "                   print(\"🧠 No new dish detected — keeping last_dish unchanged.\")\n",
        "\n",
        "                # 💬 Update chat history\n",
        "                chat_history.append([llm_input, response.content])\n",
        "                return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "            except Exception as e:\n",
        "               print(f\"🔥 Error in chat_with_memory(): {e}\")\n",
        "               return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "\n",
        "#         def chat_with_memory(user_message, session_id, chat_history, last_dish):\n",
        "#             try:\n",
        "#                 user_message = user_message.strip()\n",
        "#                 if not user_message:\n",
        "#                     return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#                 original_message = user_message\n",
        "\n",
        "#                 vague_phrases = [\n",
        "#                     \"how to make it\", \"how do i make it\",\n",
        "#                     \"is it healthy\", \"how much time\", \"can i add\", \"should i use\"\n",
        "#                 ]\n",
        "#                 if any(p in user_message.lower() for p in vague_phrases) and last_dish:\n",
        "#                     llm_input = f\"{user_message} (referring to {last_dish})\"\n",
        "#                 else:\n",
        "#                     llm_input = user_message\n",
        "\n",
        "#                 recipe_context = get_context_from_query(user_message)\n",
        "\n",
        "#                 response = memory_chain.invoke(\n",
        "#                     {\n",
        "#                       \"input\": user_message,\n",
        "#                       \"recipe_context\": recipe_context,\n",
        "#                       \"last_dish\": last_dish or \"none\"\n",
        "#                     },\n",
        "#                     config={\"configurable\": {\"session_id\": session_id}}\n",
        "\n",
        "\n",
        "# )\n",
        "\n",
        "\n",
        "#                 new_dish = detect_dish_with_llm(original_message)\n",
        "#                 if new_dish.lower() == \"none\":\n",
        "#                     new_dish = extract_dish_name(original_message)\n",
        "#                 if new_dish and new_dish.lower() != \"none\":\n",
        "#                     last_dish = new_dish\n",
        "\n",
        "#                 chat_history.append([user_message, response.content])\n",
        "#                 return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"🔥 Error in chat_with_memory(): {e}\")\n",
        "#                 return gr.update(value=chat_history), \"\", last_dish\n",
        "\n",
        "        send_button.click(\n",
        "            chat_with_memory,\n",
        "            inputs=[user_message, session_id, chat_history, last_dish_state],\n",
        "            outputs=[chatbot, user_message, last_dish_state]\n",
        "        )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "KtESOFbZ5Gf7",
        "outputId": "c0ccd581-3fbf-47be-b182-5141e4124c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-bed8c78fbba2>:116: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"NutriChef Chat\", value=[])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c3d6c528ec85670124.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c3d6c528ec85670124.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}